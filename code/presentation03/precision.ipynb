{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Subset\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kept the following code block for reference. We don't use `torch.quantization` - no quantization in general. With quantization, we could go very low in precision (e.g., int8), see [DoReFa](https://arxiv.org/pdf/1606.06160), for example.\n",
    "\n",
    "Here we want to observe the affect of Gradient on accurasy of precesion types.\n",
    "We use [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html) with Gradeint Norm Clipping.\n",
    "\n",
    "For now, we just use the floating-point 16 & 32 with the Global norm clipping, to observe the effect of Gradient on accuracy, taining and testing timings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using AMP(Automatic Mixed Precision) with gradients to observe the accuracy of precision types.\n",
    "Global Norm Gradient, Affect of Gradient on accuracy, training and testing timings.\n",
    "Gradient Value Clipping, Affect of different Gradients on precision FP16 & Fp32.\n",
    "\n",
    "## References\n",
    "\n",
    "https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html\n",
    "\n",
    "## On the difficulty of training Recurrent Neural Networks\n",
    "\n",
    "https://arxiv.org/abs/1211.5063\n",
    "\n",
    "## Mixed Precision Training\n",
    "\n",
    "https://arxiv.org/abs/1710.03740\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Supported engines:\", torch.backends.quantized.supported_engines)\n",
    "print(\"Currently active quantized engine:\", torch.backends.quantized.engine)\n",
    "torch.backends.quantized.engine = \"x86\"\n",
    "print(\"Now active quantized engine:\", torch.backends.quantized.engine)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "K_FOLDS = 5\n",
    "MOMENTUM = 0.9\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.01\n",
    "CLIP_VALUES = [0.01, 0.1, 0.5, 1.0, 2.0]  # Test 5 different thresholds\n",
    "MAX_GRAD_NORM = 0.1  # Global norm Gradient clipping threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Linear layers & ReLU are separate for fusing when quantizing (if someone wants to implement that)\n",
    "        # MNIST => 28x28=784\n",
    "        # Linear = fully connected, y=xA^T+b\n",
    "        # ReLU = activation for non-linearity\n",
    "        # Output is 10 logits (\"logit\" = raw output of a linear layer = pre-activation) used later with CrossEntropyLoss\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 28x28 MNIST images are flattened into 784-dim vectors\n",
    "        x = x.view(x.size(0), -1)  # [batchSize,1,28,28] to [batchSize,784]\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Training Function for One Epoch\n",
    "def trainEpoch(\n",
    "    model, train_loader, criterion, optimizer, scaler, max_grad_norm, use_fp16\n",
    "):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass with autocast if FP16\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=use_fp16):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass with gradient clipping\n",
    "        if use_fp16:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/63746182/correct-way-of-normalizing-and-scaling-the-mnist-dataset\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")\n",
    "full_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the Gradient affect different precisions type modelâ€™s accuracy and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_epochs(use_fp16):\n",
    "    kfold = KFold(n_splits=K_FOLDS, shuffle=True)\n",
    "    fold_accuracies = []\n",
    "    fold_train_times = []\n",
    "    fold_eval_times = []\n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(full_dataset)):\n",
    "        print(f\"\\n=== Fold {fold+1}/{K_FOLDS} ({'FP16' if use_fp16 else 'FP32'}) ===\")\n",
    "        # Data loaders\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            full_dataset, batch_size=BATCH_SIZE, sampler=train_subsampler\n",
    "        )\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            full_dataset, batch_size=BATCH_SIZE, sampler=val_subsampler\n",
    "        )\n",
    "\n",
    "        # Model setup\n",
    "        model = MLP().to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=use_fp16)\n",
    "        # Training loop with timing\n",
    "        fold_train_start = time.time()\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss = trainEpoch(\n",
    "                model,\n",
    "                train_loader,\n",
    "                criterion,\n",
    "                optimizer,\n",
    "                scaler,\n",
    "                MAX_GRAD_NORM,\n",
    "                use_fp16,\n",
    "            )\n",
    "            val_acc = evaluate(model, val_loader)\n",
    "        fold_train_end = time.time()\n",
    "\n",
    "        # Evaluation timing\n",
    "        fold_eval_start = time.time()\n",
    "        final_acc = evaluate(model, val_loader)\n",
    "        fold_eval_end = time.time()\n",
    "\n",
    "        # Store metrics\n",
    "        fold_accuracies.append(final_acc)\n",
    "        fold_train_times.append(fold_train_end - fold_train_start)\n",
    "        fold_eval_times.append(fold_eval_end - fold_eval_start)\n",
    "\n",
    "    return {\n",
    "        \"accuracies\": fold_accuracies,\n",
    "        \"train_times\": fold_train_times,\n",
    "        \"eval_times\": fold_eval_times,\n",
    "    }\n",
    "\n",
    "\n",
    "fp16_results = kfold_epochs(use_fp16=True)\n",
    "fp32_results = kfold_epochs(use_fp16=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"FP16 + Clipping\", \"FP32 + Clipping\"]\n",
    "accuracies = [np.mean(fp16_results[\"accuracies\"]), np.mean(fp32_results[\"accuracies\"])]\n",
    "train_times = [\n",
    "    np.mean(fp16_results[\"train_times\"]),\n",
    "    np.mean(fp32_results[\"train_times\"]),\n",
    "]\n",
    "eval_times = [np.mean(fp16_results[\"eval_times\"]), np.mean(fp32_results[\"eval_times\"])]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "bars = plt.bar(methods, accuracies, color=[\"skyblue\", \"lightgreen\"])\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Model Accuracy Comparison\")\n",
    "plt.ylim(90, 100)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height,\n",
    "        f\"{height:.2f}%\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(methods))\n",
    "train_bars = plt.bar(\n",
    "    index, train_times, bar_width, label=\"Training Time\", color=\"orange\"\n",
    ")\n",
    "eval_bars = plt.bar(\n",
    "    index + bar_width, eval_times, bar_width, label=\"Eval Time\", color=\"salmon\"\n",
    ")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.title(\"Training and Evaluation Times\")\n",
    "plt.xticks(index + bar_width / 2, methods)\n",
    "plt.legend()\n",
    "\n",
    "for bars in [train_bars, eval_bars]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height,\n",
    "            f\"{height:.2f}s\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(\n",
    "    f\"FP16 + Clipping - Accuracy: {accuracies[0]:.2f}% | Train Time: {train_times[0]:.2f}s | Eval Time: {eval_times[0]:.2f}s\"\n",
    ")\n",
    "print(\n",
    "    f\"FP32 + Clipping - Accuracy: {accuracies[1]:.2f}% | Train Time: {train_times[1]:.2f}s | Eval Time: {eval_times[1]:.2f}s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the both float-point 16 and float-point 32 Model with different Gradient Value Clipping.\n",
    "\n",
    "This should show the effect of different Gradients on precision FP16 & Fp32 model during training.\n",
    "\n",
    "Instead of scaling gradients based on their collective norm, we clips each individual gradient element to a fixed range [-clip_value, clip_value] or we can specify list of few Gradients. This ensures no single gradient component exceeds the specified magnitude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train_with_value_clipping(use_fp16, clip_value):\n",
    "    \"\"\"training function for value clipping\"\"\"\n",
    "    kfold = KFold(n_splits=K_FOLDS, shuffle=True)\n",
    "    fold_accs = []\n",
    "    fold_train_times = []\n",
    "    fold_eval_times = []\n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(full_dataset)):\n",
    "        print(f\"\\n=== Fold {fold+1}/{K_FOLDS} ({'FP16' if use_fp16 else 'FP32'}) ===\")\n",
    "        # Data loaders\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            full_dataset, batch_size=BATCH_SIZE, sampler=train_subsampler\n",
    "        )\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            full_dataset, batch_size=BATCH_SIZE, sampler=val_subsampler\n",
    "        )\n",
    "\n",
    "        # Model setup\n",
    "        model = MLP().to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=use_fp16)\n",
    "\n",
    "        # Training\n",
    "        train_start = time.time()\n",
    "        for epoch in range(EPOCHS):\n",
    "            model.train()\n",
    "            epoch_start = time.time()\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.autocast(\n",
    "                    device_type=\"cuda\", dtype=torch.float16, enabled=use_fp16\n",
    "                ):\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                if use_fp16:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_value_(model.parameters(), clip_value)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_value_(model.parameters(), clip_value)\n",
    "                    optimizer.step()\n",
    "            epoch_time = time.time() - epoch_start\n",
    "        train_time = time.time() - train_start\n",
    "\n",
    "        # Evaluation\n",
    "        eval_start = time.time()\n",
    "        acc = evaluate(model, val_loader)\n",
    "        eval_time = time.time() - eval_start\n",
    "\n",
    "        fold_accs.append(acc)\n",
    "        fold_train_times.append(train_time)\n",
    "        fold_eval_times.append(eval_time)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": np.mean(fold_accs),\n",
    "        \"train_time\": np.mean(fold_train_times),\n",
    "        \"eval_time\": np.mean(fold_eval_times),\n",
    "    }\n",
    "\n",
    "\n",
    "fp16_results = {\n",
    "    clip_val: train_with_value_clipping(True, clip_val) for clip_val in CLIP_VALUES\n",
    "}\n",
    "fp32_results = {\n",
    "    clip_val: train_with_value_clipping(False, clip_val) for clip_val in CLIP_VALUES\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results, precision):\n",
    "    plt.figure(figsize=(18, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    accs = [results[clip_val][\"accuracy\"] for clip_val in CLIP_VALUES]\n",
    "    bars = plt.bar(\n",
    "        range(len(CLIP_VALUES)),\n",
    "        accs,\n",
    "        color=\"skyblue\" if precision == \"FP16\" else \"lightgreen\",\n",
    "    )\n",
    "    plt.xticks(range(len(CLIP_VALUES)), [f\"Clip={v}\" for v in CLIP_VALUES])\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(f\"{precision} Accuracy\\n(Gradient Value Clipping)\")\n",
    "    plt.ylim(90, 100)\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            height,\n",
    "            f\"{height:.2f}%\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    train_times = [results[clip_val][\"train_time\"] for clip_val in CLIP_VALUES]\n",
    "    plt.bar(range(len(CLIP_VALUES)), train_times, color=\"orange\")\n",
    "    plt.xticks(range(len(CLIP_VALUES)), [f\"Clip={v}\" for v in CLIP_VALUES])\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.title(f\"{precision} Training Time\\n(Across {K_FOLDS} folds)\")\n",
    "    for i, t in enumerate(train_times):\n",
    "        plt.text(i, t + 0.5, f\"{t:.2f}s\", ha=\"center\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    eval_times = [results[clip_val][\"eval_time\"] for clip_val in CLIP_VALUES]\n",
    "    plt.bar(range(len(CLIP_VALUES)), eval_times, color=\"salmon\")\n",
    "    plt.xticks(range(len(CLIP_VALUES)), [f\"Clip={v}\" for v in CLIP_VALUES])\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.title(f\"{precision} Evaluation Time\\n(Across {K_FOLDS} folds)\")\n",
    "    for i, t in enumerate(eval_times):\n",
    "        plt.text(i, t + 0.05, f\"{t:.2f}s\", ha=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n=== FP16 Results ===\")\n",
    "plot_results(fp16_results, \"FP16\")\n",
    "\n",
    "print(\"\\n=== FP32 Results ===\")\n",
    "plot_results(fp32_results, \"FP32\")\n",
    "\n",
    "print(\"\\n=== FP16 Detailed Results ===\")\n",
    "for clip_val in CLIP_VALUES:\n",
    "    res = fp16_results[clip_val]\n",
    "    print(\n",
    "        f\"Clip {clip_val}: Acc={res['accuracy']:.2f}% | Train={res['train_time']:.2f}s | Eval={res['eval_time']:.2f}s\"\n",
    "    )\n",
    "\n",
    "print(\"\\n=== FP32 Detailed Results ===\")\n",
    "for clip_val in CLIP_VALUES:\n",
    "    res = fp32_results[clip_val]\n",
    "    print(\n",
    "        f\"Clip {clip_val}: Acc={res['accuracy']:.2f}% | Train={res['train_time']:.2f}s | Eval={res['eval_time']:.2f}s\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
