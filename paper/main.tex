\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{csquotes}
\usepackage{float}
\usepackage{derivative}
\usepackage{subcaption}
\usepackage{bbm}
\usepackage{siunitx}

\usepackage[
	style=nejm,
	citestyle=numeric-comp,
	sorting=none,
	doi=false,
	backend=bibtex]{biblatex}
\addbibresource{literature.bib}

\geometry{
	a4paper,
	total={6in, 9in},
	left=1.25in,
	top=1in,
}

\titleformat{\section}{\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection}{1em}{}

\title{An Experimental Analysis of the Influence of the Precision of Parameters in an Artificial Neural Network}
\author{Denise Katritschenko, Elias Reich, Sayed Abozar Sadat, Lukas Schwaiger\\
	\small University of Salzburg\\
	\small Department of Artificial Intelligence and Human Interfaces (AIHI) }

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
	\noindent
	We study how the numerical precision of weights, activations, and gradients in
	neural networks can be reduced without sacrificing too much of the performance.
	Using MNIST as a general dataset, and FashionMNIST for the decimal rounding, we
	benchmark training with reduced precision (FP64/FP32/FP16), mixed-precision
	formats, and post-training decimal rounding. We evaluate precision settings
	across all tensors during training, experiment with an inference-only casting
	to lower-precision floating point on FP64-trained models, and test Automatic
	Mixed Precision (AMP) with gradient clipping. Metrics include test accuracy,
	training/inference time, and parameter storage. We compare end-to-end
	low-precision training, inference-only quantization, and mixed-precision with
	gradient clipping, and evaluate their impact on test accuracy and efficiency.
	The results on the used test setup show that (i) 16-bit end-to-end training
	does not significantly decrease test accuracy for the MNIST dataset, (ii)
	three-decimal-place weight rounding while keeping 32-bit precision incurs
	minimal accuracy loss. These findings verify limits for memory- and
	energy-constrained usecases like inference in mobile devices.
\end{abstract}

\section{Introduction}
Artificial Neural Networks (ANNs) are a key part of today's machine learning
systems. They have made huge progress in areas like image recognition, speech
processing, and self-driving cars. However, one of the biggest challenges with
large ANNs is that they use a lot of memory and computing power. This becomes a
problem when you want to run them on devices with limited resources, such as
smartphones, embedded systems, or edge devices. \\\\ In order to solve this,
researchers have looked into ways to make neural networks more efficient. A
common approach is to reduce the size of the network or shrink the input batch
size. But doing this can harm the network's ability to learn and generalize. A
better strategy is to reduce the numerical precision of the network's
parameters such as weights, activations, and gradients. This method, called
quantization, makes the model smaller and faster while using less energy,
without necessarily hurting accuracy. \\\\ An advanced version of this method
is called mixed-precision quantization. Instead of using the same number of
bits for every layer in the network, mixed-precision assigns different
bitwidths depending on how important each part is. For example, the early
layers that handle low-level features might use higher precision, while deeper
layers could work fine with fewer bits. This flexible approach often leads to
better results than using a single fixed precision for the whole model.
Researchers have used a range of techniques, like Stochastic Gradient Descent
(SGD), heuristics, and even evolutionary algorithms, to find the best bitwidth
settings for each layer. \\\\ Another method that helps reduce resource usage
is network pruning. This means cutting out parts of the network that do not
contribute much to its performance. A way to do this is simulated annealing, a
search algorithm that mimics the process of cooling metal to find a stable
structure. It does not need gradient information and can still find good
network shapes by exploring many possibilities. This makes it useful when you
want to shrink a network without going through heavy retraining. \\\\ Previous
studies have shown that reducing the precision of neural network parameters can
lead to significant improvements in memory usage and inference efficiency
without degrading the accuracy too much. For example, integer-only INT8
quantization~\cite{jacob2018quantization} and also binary-weight and
binary-activation CNNs evaluated on ImageNet~\cite{rastegariECCV16}. However,
while these early results are promising, the accuracy/efficiency trade-off
depends on architecture, task, storage efficiency, and training setup, so
precision reduction still deserves more detailed investigation. \\\\ This is
where our experimental work begins. In this paper, we take a closer look at how
different levels of parameter precision affect the behavior and performance of
neural networks. Our goal is to understand what happens when we reduce
precision in various ways, and how much we can lower it before performance
starts to drop. \\\\ We start by testing how reducing precision during both
training and inference affects model accuracy and runtime. Next, we look at
what happens when we train the network using full precision, but lower the
precision only during inference. This setup is common in real-world
applications, where a powerful machine handles training, but the final model is
deployed on a lightweight device. Then, we run a series of tests where we round
the trained weights to a limited number of decimal places. This helps to see
how much we can simplify the stored weight values without hurting accuracy too
much. This is especially useful for devices with very limited storage. We also
try out mixed-precision training, using a mix of Double-Precision Floating
Point (FP64), Half-Precision Floating Point (FP16) and Single-Precision
Floating Point (FP32) formats. On top of that, we experiment with different
types of gradient clipping, like global norm clipping and value clipping, to
see how they affect training stability and final accuracy. \\\\ Even though we
use relatively simple network architectures and datasets, our results show that
neural networks are quite robust to low-precision settings. In some cases, we
were able to round weights to just three decimal digits and still get nearly
the same accuracy as the full-precision version. These findings show that it is
possible to significantly reduce the memory and compute needs of neural
networks without losing much performance. This makes them more practical for
use in low-resource settings. It also opens the door for more advanced
techniques that combine quantization, pruning, and precision-aware training to
build highly efficient models for real-world applications.

\subsection{Related Work}
Prior research in this domain can be broadly divided into methods that use
quantization already during training and methods that quantize a pretrained
model later at deployment time.

\subsubsection*{Quantization During Training}
Mixed-precision training is an approach where most operations are performed in
FP16, but a separate high-precision copy of the parameters (often called
\textit{master weights}) is maintained in FP32~\cite{micikevicius2017mixed}.
The forward and backward passes use FP16 tensors for faster computation and
reduced memory usage, but parameter updates are applied to the FP32 master
weights. The updated FP32 weights are then cast back to FP16 for the next
forward pass. Keeping master weights in higher precision keeps the loss of
information smaller and helps the training stability.\\

\noindent
DoReFa-Net extends this concept by quantizing \textit{weights},
\textit{activations}, and \textit{gradients} to low-precision
formats~\cite{zhou2018dorefa}. The method allows using \textit{arbitrary
	bitwidths} which means the number of bits is not restricted to standard IEEE
floating-point formats, but can be any small integer (for example, 1, 2, or 4
bits for weights and slightly higher bitwidths for activations and gradients).
This is flexible and makes it possible to set the precision specific to
hardware constraints or application requirements, but the training of the
network is still end-to-end.\\

\noindent
Another work is Q-BERT~\cite{shen2020qbert}, that focuses on transformer-based
language models. BERT (Bidirectional Encoder Representations from Transformers)
is a large transformer architecture pretrained on large text data and used as a
generic language representation model. Q-BERT shows that the weights of BERT
can be quantized down to 4 bits while maintaining accuracy close to the
full-precision model.

\subsubsection*{Post-Training Quantization}
Post-training quantization begins with an already trained full-precision model
and converts it to a lower precision without (or with minimal) additional
training. An often used approach is integer quantization with symmetric
scaling~\cite{jacob2018quantization}. Here, both weights and activations are
mapped to signed integers (for example 8-bit) using a scale factor that is the
same across positive and negative values (therefore symmetric). This design
allows the entire inference flow to be implemented with integer arithmetic,
which is better for embedded or mobile hardware.\\

\noindent
Methods such as GPTQ (Gradient Post-Training Quantization) have been tested to
quantize large transformer models. Examples are GPT language models with
minimal or no retraining~\cite{frantar2022gptq}. GPTQ handles quantization as
an optimization problem by using approximations of the local curvature to
choose quantization levels that minimize the error introduced by replacing
full-precision weights with low-precision ones. That makes this approach better
for very large models where a full retraining with quantization would be too
expensive.\\

\noindent
SmoothQuant focuses more on activations~\cite{xiao2024smoothquant}. It rescales
activations and compensates for this rescaling in the weight matrices so that
the activation distributions become more smooth and better aligned with the
range supported by low-precision integer formats. This improves the
post-training quantization, especially in transformer architectures where
activation outliers can otherwise complicate the scaling and lead to large
quantization errors.

\subsection{Artificial Neural Networks}
A typical neuron in a neural network performs a weighted sum of its inputs,
followed by an activation function which results in the neuron output $h$
depicted in~\eqref{eq:activation}. Here, $\omega\in\mathbb{R}^n$ is the weight
vector, $b \in \mathbb{R}$ is the bias, $x \in \mathbb{R}^n$ is the input, and
$\sigma$ is a nonlinear activation function (sigmoid or ReLU, for example).

\begin{equation}\label{eq:activation}
	a=\omega^\top x+b,\quad h=\sigma(a)
\end{equation}

\noindent
In our MLP, hidden layers use the Rectified Linear Unit (ReLU), so
$\sigma=\mathrm{ReLU}$, while the output layer is linear, producing raw scores
(also called logits) $z$ later used in~\eqref{eq:ce}. ReLU~\eqref{eq:relu}
keeps positive activations linear and sets negatives to zero. An additional
benefit is that the calculation is very fast and simple compared to more
complex activation functions.

\begin{equation}\label{eq:relu}
	\mathrm{ReLU}(a)=\max(0,a),\qquad\frac{d}{da}\mathrm{ReLU}(a)=\mathbbm{1}\{a>0\}
\end{equation}

\noindent
Training of the network is conducted via backpropagation, where gradients of a
loss function $L$ are computed with respect to the model parameters. For
multi-class classification with $C=10$ classes, the last layer outputs logits
$z\in\mathbb{R}^C$. To convert the logits into probabilities, usually softmax
defined as $p_k=\frac{e^{z_k}}{\sum_{j=1}^C e^{z_j}}$ is used. The probability
of the true class $y$ is then $p_y$. In our experiments, though, we train
directly on the logits using a simplified version of the cross-entropy loss
(the negative log-likelihood of a softmax classifier) implemented in
Pytorch\footnote{\url{https://pytorch.org/}}:

\begin{equation}\label{eq:ce}
	L(z,y)=-\log p_y=-z_y+\log\sum_{j=1}^C e^{z_j},
\end{equation}

whose gradients are

\begin{equation}\label{eq:cegrads}
	\frac{\partial L}{\partial z_k}=p_k-\mathbbm{1}\{k=y\},\qquad\frac{\partial L}{\partial W}=(p-e_y)h^\top,\qquad\frac{\partial L}{\partial b}=p-e_y,
\end{equation}

with $W$ and $b$ the weight matrix and bias of the last linear layer, $h$ the
layer input, and $e_y$ the one-hot vector of the target class. Here,
$\mathbbm{1}\{k=y\}$ is the indicator: it equals \num{1} if $k=y$ and \num{0}
otherwise.\\

\noindent
Memory and computational requirements during training are substantial, as
weights, gradients, and activations must be stored and updated iteratively.
Training and inference can both be optimized using precision reduction and
related techniques, but inference is more well suited to optimizations because
it only requires a forward pass.

\subsection{Memory Efficiency and Numerical Formats}
To reduce memory usage and computational load, precision-aware techniques such
as quantization and mixed-precision training are increasingly employed. Instead
of using 32-bit or 64-bit floating-point formats, parameters can be stored in
16-bit or even 8-bit fixed-point representations.
Equation~\ref{eq:floatingPointFormat} depicts the general floating point format
and terminology of significand, base, and exponent.

\begin{align}\label{eq:floatingPointFormat}
	\frac{2469}{200}=12.345=\underbrace{12345}_{\text{significand}}\times{{\underbrace{10}_{\text{base}}}^{\overbrace{-3}^{\text{exponent}}}}
\end{align}

\noindent
Tables~\ref{tab:binary-bits} and~\ref{tab:binary-precision} provide an overview
of the different floating point types and their respective characteristics. The
\textit{sign}, \textit{exponent}, and \textit{significand} fields specify how many
bits are allocated to encode the sign of the number, its scale, and its
significant digits, respectively. The \textit{exponent bias} is a constant added
to the true exponent so that it can be stored as an unsigned integer. The
\textit{precision} (in bits) and corresponding \textit{decimal digits} tell how
many significant digits can be represented accurately in each format.

\begin{table}[H]
	\centering
	\begin{tabular}{@{}lcccc@{}}
		\toprule
		\textbf{Type} & Sign & Exponent & Significand & Total \\
		\midrule
		Half  (FP16)  & 1    & 5        & 10          & 16    \\
		Single (FP32) & 1    & 8        & 23          & 32    \\
		Double (FP64) & 1    & 11       & 52          & 64    \\
		\bottomrule
	\end{tabular}
	\caption{Field widths (in bits) of common IEEE754 formats (adapted from~\cite{fparithmetic}).}\label{tab:binary-bits}
\end{table}

\begin{table}[H]
	\centering
	\begin{tabular}{@{}lccc@{}}
		\toprule
		\textbf{Type} & Exponent bias & Precision (bits) & Decimal digits \\
		\midrule
		Half (FP16)   & 15            & 11               & $\sim$3.3      \\
		Single (FP32) & 127           & 24               & $\sim$7.2      \\
		Double (FP64) & 1023          & 53               & $\sim$15.9     \\
		\bottomrule
	\end{tabular}
	\caption{Range and precision characteristics of the formats (adapted from~\cite{fparithmetic}).}\label{tab:binary-precision}
\end{table}

\section{Experimental Design}
The effect of numerical precision on training and inference is evaluated
through five experiment groups run on standard image classification tasks. The
main dataset MNIST\footnote{LeCun, Cortes, and Burges:
	\url{http://yann.lecun.com/exdb/mnist/}.} ($28\times 28$ grayscale images of
handwritten digits, train/test split \num{60000}/\num{10000}, \num{10} classes,
examples shown in Figure~\ref{fig:mnistSample}) is used to determine precision
effects on accuracy and runtime under a simple workload, while an
after-training rounding experiment is performed on FashionMNIST\footnote{Xiao,
	Rasul, and Vollgraf: \url{https://github.com/zalandoresearch/fashion-mnist}.}
($28\times 28$ grayscale images of clothing items, train/test split
\num{60000}/\num{10000}, \num{10} classes) so the impact of using coarse value
sets is better observable.\\

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{figures/mnist.png}
	\caption{Sample images from the MNIST dataset.}\label{fig:mnistSample}
\end{figure}

\noindent
The classifier used is a fully connected network (layers:
$784\rightarrow128\rightarrow64\rightarrow10$) with ReLU between linear layers
(see structure in Figure~\ref{fig:mlpStructure}). The final linear layer
produces \num{10} logits and during training we use \textit{CrossEntropyLoss}
on the logits, softmax is not applied in the forward pass. Optimization uses
SGD with learning rate \num{0.01} and momentum \num{0.9}, batch size \num{64},
on CUDA.\ Unless noted otherwise we train for a small, fixed number of epochs
per fold. Automatic mixed precision runs use \textit{autocast} with a
\textit{GradScaler}. When enabled, gradients are clipped either by global norm
or per-element value as specified in Section~\ref{sec:results}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{figures/mlpStruc.png}
	\caption{Network structure used in the experiments.}\label{fig:mlpStructure}
\end{figure}

\noindent
For the MNIST experiments (i) end-to-end precision, (ii) timing, (iii)
inference-only precision reduction, and (iv) AMP with gradient clipping, we use
5-fold cross-validation on the training set with random equal splits. Unless
noted otherwise, figures report fold means, and no test set tuning is
performed. In the end-to-end runs, every floating point tensor appearing inside
the training loop (parameters, activations, gradients) is kept at the target
precision during training and evaluation. The timing experiment reuses these
runs to measure mean per-epoch training time and mean evaluation time. For
inference-only reduction, we first train a baseline model in FP64 and then do
inference on copies cast to FP32 and FP16 without any further optimization (so
no \textit{retraining} on the lower precision).\\

The AMP experiment is run on MNIST with gradient clipping, comparing an AMP
setup (FP16 computations with FP32 master weights) against a pure FP32
baseline. We try different global-norm and per-element value-clipping
thresholds to check stability and report test accuracy and training/inference
times. We also run a one-off rounding experiment on FashionMNIST (batch size
\num{256}, FP32 training): after training, only the stored weights are rounded
to $k$ decimal places ($k{=}1\ldots14$) and inference remains FP32. Inputs and
intermediate computations stay at full precision. We report test accuracy as a
function of $k$ and note the impact on storage.

\section{Experiments and Results}\label{sec:results}
In the first experiment, every floating-point tensor that is used inside the
training loop was converted to a lower precision data type. This therefore
includes the weights and biases, activations, gradients, optimizer states, etc.
The types tried were FP64, FP32, and FP16.

\subsection*{Accuracy Across Bitwidths}
Figure~\ref{fig:modelAccTrainAndInf} shows how the accuracy changed after the
models were trained on the different precision settings. Keep in mind that the
y-axis is not starting from zero, so the absolute height differences are
amplified visually, though the delta is only around $.1\%$. Retraining the
models and re-evaluating them has also lead to the accuracies being
``reversed'', meaning the FP64 model had the least accuracy. It is safe to
assume that the model accuracy has neither decreased, nor increased.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{figures/trainingAndInference.png}
	\caption{Model accuracy evaluated after training with different precisions.}\label{fig:modelAccTrainAndInf}
\end{figure}

\subsection*{Training and Inference Time}
We also analyzed the times needed for both, training and inference.
Figure~\ref{fig:modelEvalTrainTimes} depicts the times measured in the training
loop and later on at the inference stage. Again, the times are almost identical
in both regards, and neither a real speed-up nor a slow-down has been measured.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{figures/times.png}
	\caption{Training and evaluation times for the different precisions.}\label{fig:modelEvalTrainTimes}
\end{figure}

\subsection*{Inference-Only Testing}
This third experiment was conducted to test a possible post-training precision
reduction. For this, the model was trained on the highest precision (FP64) and
then quantized at the inference stage. Figure~\ref{fig:inferenceOnlyTesting}
shows that the accuracy of the baseline (FP64 training AND inference) is
maintained as the precision is decreased at the later stages. In contrast to
the \textit{Accuracy Across Bitwidths} experiment, there are no fluctuations at
all because for the lower precisions, there was no new model trained.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{figures/inferenceOnly.png}
	\caption{Inference accuracy using FP64-trained weights followed by quantization.}\label{fig:inferenceOnlyTesting}
\end{figure}

\subsection*{Automatic Mixed Precision and Gradient Clipping}
Automatic Mixed Precision (AMP) improves deep learning training by using both
FP16 and FP32 data types reducing memory usage and increasing computational
speed. However, FP16's limited dynamic range can cause numerical instability,
especially during backpropagation. To address this, Gradient Clipping is used
to cap the magnitude of gradients (either globally or per element) preventing
extreme updates that could hinder convergence. Together, AMP and gradient
clipping enable efficient, stable training by balancing performance and
numerical precision.

First step is using global norm clipping, all gradients are collectively scaled
to ensure their combined norm does not exceed a specified threshold, promoting
stable updates across the network. In second step per-element clipping
constrains each individual gradient component within a fixed range, directly
limiting extreme values. These methods are evaluated under AMP to compare their
impact on training stability, model accuracy, and computation time across
different precision types.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{figures/FP16_32_clipping.png}
	\caption{Affect of Gradient Norm on accuracy, training and testing timings.
		Accuracy is virtually the same between FP16 and FP32 when gradient clipping is used
		validating the stability of AMP and gradient scaling.
		Eval time is nearly identical for both formats, as evaluation involves fewer operations
		and less intense memory bandwidth usage.}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{figures/gradclip_16.png}
	\caption{Effect of Per-Element Gradient Clipping on FP16. Accuracy vs Training \& Eval Time}\label{fig:grad16}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{figures/gradclip_32.png}
	\caption{Effect of Per-Element Gradient Clipping on FP32. Accuracy vs Training \& Eval Time}\label{fig:grad32}
\end{figure}

Figure~\ref{fig:grad16} shows accuracy peaked at 97.07\% with a clipping value
of 0.5, while the lowest accuracy (96.55\%) occurred at clip=0.01, suggesting
that very small clip values may overly restrict gradient updates and hurt
learning. Training time ranged indicating that moderate clipping supports
efficient convergence. Evaluation time remained stable across all clip values
(around 2.01s to 2.16s), showing that clipping primarily impacts training
rather than inference performance. Also Figure~\ref{fig:grad32} shows that
accuracy was most stable across all clip values with the highest accuracy at
clip=0.5, demonstrating its robustness to gradient clipping variations.
Training time suggesting that larger clip values can speed up convergence
without sacrificing stability. Evaluation time confirming that clipping had
minimal effect on inference time.

\subsection*{Effect of Decimal Rounding}

A non training related issue is the storage of the NN parameters. Usually, the
parameters are stored in the format that was chosen to train with. This is best
for reproducibility as well as accuracy when doing inference with the trained
model weights. The datatype precision has a higher impact during training
though, which begs the question if one could omit precision from the model
weights. In this section, we explore a naive quantization method: Rounding to a
fixed number of decimals. With significantly little decimals and a reasonable
range for the integer part, one could store only the significant and provide
base as well as exponent as metadata. When loading the model, the weights are
converted back to floating point, with whatever precision is desired. Only the
weights are quantized; the input data, as well are the values propagated
through the network are at full precision.

The first step is to examine the distribution of parameter values. By the
standard deviation around modes one can make an educated guess as to how many
decimals are needed. The range will decide how large the integer part may get.
Reading from the distribution is just a heuristic though, so we will also test
the hypotheses.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{figures/118_dist.png}
	\caption{
		Distribution of model weights in each layer. The values are in $[-0.5, 0.5]$,
		this means no integer part is needed in this example. Generally, with normalized data
		and the quite popular L1-/L2-/Batch-Norm layers, weight values in $[-1, 1]$ is to be expected.}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{figures/118_dist2.png}
	\caption{Distribution of model weights in each layer after rounding to two decimals.
		This appears to be a good approximation already.}\label{fig:118_2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{figures/118_accuracy.png}
	\caption{Test accuracy vs.\ number of decimal digits retained in weights.}\label{fig:118_acc}
\end{figure}

Figure~\ref{fig:118_acc} shows how rounding the decimal part affects the test
accuracy. For just one decimal the model has a maximum of 19 different weights
at its disposal, which makes the model have only 10\% accuracy, random guessing
with a 10 class problem. Two decimals perform significantly better. At 70\%
accuracy the model gets close to the full precision 88\% accuracy with 32 bit
floating point. Making an educated guess on distribution such as
Figure~\ref{fig:118_2} seems to be a good heuristic for this example. At just 3
decimals, the full 32 bit precision is reached. Thus we can conclude that with
just 10 bit for representing 3 decimals, no bits for the integer part and one
additional bit for the sign, only 11 bit must be stored to have the best
performing model for inference.

\section{Summary}

The experimental results presented in this paper demonstrate the robustness of
neural networks to reduced parameter precision, particularly for simpler tasks
like MNIST classification. Key findings include:

\begin{itemize}
	\item \textbf{Precision Reduction During Training and Inference}: Training and
	      evaluating models with 16-bit floating-point (FP16) precision preserved accuracy
	      comparable to higher-precision formats (FP32 and FP64). This suggests that for
	      tasks of similar complexity, FP16 is sufficient, offering potential memory and
	      computational savings without sacrificing performance.

	\item \textbf{Post-Training Quantization}: Reducing precision only during inference, after
	      training with high-precision (FP64), maintained baseline accuracy. This is
	      particularly valuable for deployment scenarios where training is done on powerful
	      hardware, but inference occurs on resource-constrained devices.

	\item \textbf{Mixed-Precision:} Training with mixed precision and Gradient Clipping
	      resulted in similar test accuracy scores as precision reduction during training
	      as well as post-training quantization. No significant decrease in model
	      performance accuracy was detected. The train and test times were similar,
	      regardless of using FP32, FP16 or a mix of both.

	\item \textbf{Weight Rounding:} Rounding trained weights to just three decimal digits
	      (approximately 11 bits) achieved full accuracy, highlighting the potential for
	      aggressive quantization in storage-efficient model deployment. This finding is
	      significant for edge applications where memory footprint is critical.
\end{itemize}

However, the simplicity of the MNIST dataset and the small network architecture
used in the experiments may limit the generalizability of these findings. More
complex tasks (e.g., CIFAR-10, ImageNet) and deeper architectures (e.g., CNNs,
Transformers) could exhibit greater sensitivity to precision reduction.
Additionally, the impact of precision on training dynamics, such as gradient
flow and convergence, was not deeply explored and could be a focus for future
work.

\section{Conclusion}

This study demonstrates that neural networks can maintain strong performance
even under significant reductions in parameter precision, particularly for
simpler tasks like MNIST classification. Key findings confirm that FP16
training and inference, post-training quantization, and aggressive weight
rounding (down to three decimal digits) preserve model accuracy while improving
memory and computational efficiency. These results are promising for deploying
models in resource-constrained environments, such as edge devices and real-time
applications.

To build on these insights and expand their applicability, future work should
focus on the following directions:

\begin{itemize}
	\item \textbf{Complex Architectures and Datasets:} Future research should validate these
	      findings on more complex models (e.g., CNNs, Transformers) and datasets (e.g., CIFAR-10,
	      ImageNet) to assess the scalability of low-precision techniques.

	\item \textbf{Quantization-Aware Training:} Investigating advanced methods like learned
	      quantization, non-uniform bit allocation, and dynamic precision adjustment could further
	      optimize the trade-off between efficiency and accuracy.

	\item \textbf{Hardware-Specific Optimizations:} Exploring how reduced precision interacts
	      with specialized hardware (e.g., TPUs, neuromorphic chips) could unlock new efficiency
	      gains in real-world deployments.

	\item \textbf{Theoretical Understanding:} A deeper analysis of how precision affects
	      gradient dynamics, convergence, and generalization could provide foundational insights for
	      designing robust low-precision training algorithms.
\end{itemize}

\newpage
\printbibliography%
\end{document}
